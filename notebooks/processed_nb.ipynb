{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JnWK0O1TKHJG"
      },
      "source": [
        "# **CSC 418 Group D Data Science Project**\n",
        "\n",
        "Category : **Major**\n",
        "Group Members\n",
        " \n",
        "1. P15/5620/2019 : Njagi Baraka Fadhili\n",
        "2. P15/1636/2019 : Kabiru Sharleen Njeri\n",
        "3. P15/1635/2019 : Obora Melanie Fayne\n",
        "4. P15/137631/2019 : Ali Amina Abdi\n",
        "5. P15/130607/2018 : Munyao Mary June"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FaDKI7E3LKl8"
      },
      "source": [
        "## Importing libraries\n",
        "_____"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKKXquTlHwBC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Loading Model libraries\n",
        "import sys\n",
        "!{sys.executable} -m pip install xgboost\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from lightgbm import LGBMClassifier\n",
        "from scipy.special import erfc\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score,accuracy_score,classification_report,confusion_matrix , recall_score, precision_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, log_loss\n",
        "\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "np.random.seed(2017)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1mwZtHRhLoJR"
      },
      "source": [
        "## Reading Files\n",
        "_____"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "hHdPBpgSLsDI",
        "outputId": "854f099b-bc63-4bcd-d169-ff7c14ab0b39"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"../data/train.csv\")\n",
        "# Preview the first five rows of the train dataset\n",
        "print(f'The shape of the dataset is: {train_data.shape}')\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "5XKpPb_773X0",
        "outputId": "07406d6b-abee-48ee-9491-46b0c918b1f6"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(\"../data/test.csv\")\n",
        "# Preview the first five rows of the test dataset\n",
        "print(f'The shape of the dataset is: {test_data.shape}')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldZwYz2oEr-N"
      },
      "source": [
        " *   We are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column\n",
        "\n",
        "* The train data and test data  has 4992 unique Columns \n",
        "* the train data has 4459 rows \n",
        "* the test data has 49342 rows \n",
        "* In the Train data , the Number of columns is more than the number of train rows.\n",
        "* Test data is almost 10 times as that of train set.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tByDcablMLuE"
      },
      "source": [
        "## Data Understanding and Preparation\n",
        "_____"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RfSnR9OSbaBv"
      },
      "source": [
        "**Things to check for**:\n",
        "\n",
        "1. Check for the unique data and unqiue column name in the train and test data \n",
        "2. Check for null and duplicate values\n",
        "3. Check for Outliers\n",
        "4. Check for Feature Distribution\n",
        "5. Check for Feature Importance: Through corellation and collinearity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nPJthYH3Incg"
      },
      "source": [
        "### Checking for Data Uniqueness"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### In the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "L8sNWP2AHBcN",
        "outputId": "d646852b-8f1b-4b65-f138-50281ac9e1ad"
      },
      "outputs": [],
      "source": [
        "# Check unique data in the train dataset columns \n",
        "unique_df = train_data.nunique().reset_index()\n",
        "unique_df.columns = [\"col_name\", \"unique_count\"]\n",
        "unique_df = unique_df.sort_values(\"unique_count\")\n",
        "unique_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS1c5vaPHSch",
        "outputId": "0878115e-4f13-4c47-ef12-1db864998553"
      },
      "outputs": [],
      "source": [
        "# As we can see there column with any one unique value present\n",
        "# Lets print the no of columns with 1 unique values \n",
        "constant_col = unique_df[unique_df[\"unique_count\"]==1]\n",
        "constant_col.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TCDmTXaH_4f",
        "outputId": "62aa7036-d95c-4f90-dd73-917dd0214659"
      },
      "outputs": [],
      "source": [
        "#remove this constant col \n",
        "print('Original Shape of Train Dataset {}'.format(train_data.shape))\n",
        "train_data.drop(constant_col.col_name.tolist(), axis = 1, inplace = True)\n",
        "print('Shape after dropping Constant Columns from Train Dataset {}'.format(train_data.shape))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zJr1jb1TIhgv"
      },
      "source": [
        "#### In the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "L0c5Jl6kI6Lv",
        "outputId": "5b98a902-48ed-4115-d4ac-c85b4307a3a3"
      },
      "outputs": [],
      "source": [
        "#check unique data in the column \n",
        "unique_df = test_data.nunique().reset_index()\n",
        "unique_df.columns = [\"col_name\", \"unique_count\"]\n",
        "unique_df = unique_df.sort_values(\"unique_count\")\n",
        "unique_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bJc4QfJDL8",
        "outputId": "9f5d9315-2b55-47be-8eda-ae043f5ba1ac"
      },
      "outputs": [],
      "source": [
        "constant_col = unique_df[unique_df[\"unique_count\"]==1]\n",
        "constant_col.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4lyUQLJHQc"
      },
      "source": [
        "**Observation**: \n",
        "1. The train data has 256 constant columns \n",
        "2. The test data has 0 constant columns "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "murocXamJRoH"
      },
      "source": [
        "### Checking for Duplicate Features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1MuSHq5JZut",
        "outputId": "bad593d9-d536-46ed-d926-b28939e236cc"
      },
      "outputs": [],
      "source": [
        "# get the boolean array of duplicate column names in the train dataset\n",
        "duplicate_col = train_data.columns.duplicated()\n",
        "\n",
        "# check if there are any duplicate column names\n",
        "if any(duplicate_col):\n",
        "    print(\"There are duplicate column names\")\n",
        "else:\n",
        "    print(\"All column names are unique\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC_NVlEHMtxG",
        "outputId": "5eeef906-d4fe-45dc-90f3-940872e331ee"
      },
      "outputs": [],
      "source": [
        "# get the boolean array of duplicate column names in the test dataset\n",
        "duplicate_col = test_data.columns.duplicated()\n",
        "\n",
        "# check if there are any duplicate column names\n",
        "if any(duplicate_col):\n",
        "    print(\"There are duplicate column names\")\n",
        "else:\n",
        "    print(\"All column names are unique\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-AMtvmfuMyzt"
      },
      "source": [
        "**Observation**: The train and test data have unique columns names "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wDjhLTqSl9YR"
      },
      "source": [
        "### Reducing dimensionality using"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yx5VwGo-FrqR"
      },
      "source": [
        "Dimensionality reduction is the process of reducing the number of features or variables in a dataset while preserving as much of the important information as possible. This can be useful for visualizing high-dimensional data, reducing the computational cost of modeling, and avoiding overfitting\n",
        "\n",
        "\n",
        "Autoencoder is an unsupervised neural network that learns to reconstruct the input data by compressing it into a lower-dimensional representation (encoding) and then decompressing it back to its original form (decoding). It can be used for dimensionality reduction by using the encoded representation as a new feature space.\n",
        "\n",
        "**Steps involved**: \n",
        "* Prepare Data\n",
        "* Design Auto Encoder\n",
        "* Train Auto Encoder\n",
        "* Use Encoder level from Auto Encoder\n",
        "* Use Encoder to obtain reduced dimensionality data for train and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGVTH0fJQpAd"
      },
      "outputs": [],
      "source": [
        "# lets first create a copy of the train and test data \n",
        "train_df = train_data.copy()\n",
        "test_df = test_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCWnuhsAFxUV",
        "outputId": "81a262b6-92ad-4e62-978b-6c15f2afea8a"
      },
      "outputs": [],
      "source": [
        "# drop the target and id column from the train data and test data to protect them from encoding\n",
        "train_df.drop(train_df[['ID', 'target']], axis=1, inplace=True)\n",
        "test_df.drop(test_df[['ID']], axis=1, inplace= True)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Ahr0wCIFAh"
      },
      "outputs": [],
      "source": [
        "# scale the train and test data for neural network  \n",
        "# Create the scaler object\n",
        "scaler = StandardScaler()\n",
        "# Scale the train data data\n",
        "train_scaled = scaler.fit_transform(train_df )\n",
        "test_scaled = scaler.fit_transform(test_df )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFyZ0t9CJfLS"
      },
      "outputs": [],
      "source": [
        "#design the autoencoder \n",
        "#split the train data in train and test \n",
        "import numpy as np\n",
        "np.random.seed(2017)\n",
        "X_train, X_test = train_test_split(train_scaled, train_size = 0.9, random_state = np.random.seed(2017))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzzYbjPNS5zi"
      },
      "outputs": [],
      "source": [
        "# Defining the input layer \n",
        "col_no = train_scaled.shape[1]\n",
        "input_dim = Input(shape = (col_no, ))\n",
        "\n",
        "# Defining the encoder dimension\n",
        "encoding_dim = 200\n",
        "\n",
        "# Creating  Encoder Layers\n",
        "encoded1 = Dense(3000, activation = 'relu')(input_dim) #apply to the previous layer \n",
        "encoded2 = Dense(2750, activation = 'relu')(encoded1)\n",
        "encoded3 = Dense(2500, activation = 'relu')(encoded2)\n",
        "encoded4 = Dense(2250, activation = 'relu')(encoded3)\n",
        "encoded5 = Dense(2000, activation = 'relu')(encoded4)\n",
        "encoded6 = Dense(1750, activation = 'relu')(encoded5)\n",
        "encoded7 = Dense(1500, activation = 'relu')(encoded6)\n",
        "encoded8 = Dense(1250, activation = 'relu')(encoded7)\n",
        "encoded9 = Dense(1000, activation = 'relu')(encoded8)\n",
        "encoded10 = Dense(750, activation = 'relu')(encoded9)\n",
        "encoded11 = Dense(500, activation = 'relu')(encoded10)\n",
        "encoded12 = Dense(250, activation = 'relu')(encoded11)\n",
        "encoded13 = Dense(encoding_dim, activation = 'relu')(encoded12)\n",
        "\n",
        "# Creating the Decoder Layers\n",
        "decoded1 = Dense(250, activation = 'relu')(encoded13)\n",
        "decoded2 = Dense(500, activation = 'relu')(decoded1)\n",
        "decoded3 = Dense(750, activation = 'relu')(decoded2)\n",
        "decoded4 = Dense(1000, activation = 'relu')(decoded3)\n",
        "decoded5 = Dense(1250, activation = 'relu')(decoded4)\n",
        "decoded6 = Dense(1500, activation = 'relu')(decoded5)\n",
        "decoded7 = Dense(1750, activation = 'relu')(decoded6)\n",
        "decoded8 = Dense(2000, activation = 'relu')(decoded7)\n",
        "decoded9 = Dense(2250, activation = 'relu')(decoded8)\n",
        "decoded10 = Dense(2500, activation = 'relu')(decoded9)\n",
        "decoded11 = Dense(2750, activation = 'relu')(decoded10)\n",
        "decoded12 = Dense(3000, activation = 'relu')(decoded11)\n",
        "decoded13 = Dense(col_no, activation = 'sigmoid')(decoded12)\n",
        "\n",
        "# Creating the autoenconder\n",
        "# The combined Encoder and Deocder layers input will be the input dim layer and output is the decode layer \n",
        "autoencoder = Model(inputs = input_dim, outputs = decoded13)\n",
        "\n",
        "# Compiling the Model\n",
        "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCP9JLXXTUaf",
        "outputId": "7b90b8a3-23f9-40f4-c779-364e1eab16c3"
      },
      "outputs": [],
      "source": [
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fxnTudBTfjS",
        "outputId": "cbb15342-acf5-4081-eb25-97029aefa282"
      },
      "outputs": [],
      "source": [
        "# Once the autoencoder is compiled, we train it using the training dataset.\n",
        "autoencoder.fit(X_train, X_train, epochs = 10, batch_size = 32, shuffle = False, validation_data = (X_test, X_test))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "avvn8FrdHrn8"
      },
      "source": [
        "Using the encoder to reduce dimensionality:\n",
        "* Once the autoencoder is trained, you can use the encoder part of the autoencoder to reduce the dimensionality of the dataset. By calling the predict() function on the encoder, you can transform the input data to a lower-dimensional representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qfOYtAoAxSP"
      },
      "outputs": [],
      "source": [
        "# We use the autoencoder to reduce the dimension of the train and test data \n",
        "encoder = Model(inputs = input_dim, outputs = encoded13)\n",
        "encoded_input = Input(shape = (encoding_dim, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQJGEt1RBDKy",
        "outputId": "03e64de7-d70b-4b51-96ef-f6bebc2e1627"
      },
      "outputs": [],
      "source": [
        "# Predict the new train and test using the autoencoder \n",
        "new_train = pd.DataFrame(encoder.predict(train_scaled))\n",
        "new_train = new_train.add_prefix('feature_')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9qBpEEyOwIn"
      },
      "outputs": [],
      "source": [
        "new_test = pd.DataFrame(encoder.predict(test_scaled))\n",
        "new_test = new_test.add_prefix('feature_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "CjTaPjhVBVty",
        "outputId": "b73fc850-37ca-4e35-9d8f-a1a77b0b7f50"
      },
      "outputs": [],
      "source": [
        "# We then add back the target and the Id code we droped earlier \n",
        "train_df1 = pd.concat([train_data[['ID', 'target']], new_train], axis=1)\n",
        "print(train_df1.shape)\n",
        "train_df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPlTBAtNBfmN"
      },
      "outputs": [],
      "source": [
        "# Viewing the shape of the new test data \n",
        "test_df1 = pd.concat([test_data[['ID']], new_train], axis=1)\n",
        "print(test_df1.shape)\n",
        "test_df1.head()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0hC9Zb1WEbGI"
      },
      "source": [
        "### Checking for nulls and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sk_RjJraxqt"
      },
      "outputs": [],
      "source": [
        "train_df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG_JQZj1J4k0"
      },
      "outputs": [],
      "source": [
        "test_df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OdlnAXHeNcz"
      },
      "outputs": [],
      "source": [
        "# describing numerical values\n",
        "train_df1.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KUp-n4dYxfr"
      },
      "outputs": [],
      "source": [
        "# Categorical Values/Object Values\n",
        "train_df1.describe(include=\"O\").T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gVxsT1tNBwd"
      },
      "outputs": [],
      "source": [
        "test_df1.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4dwhDS0KCE8"
      },
      "outputs": [],
      "source": [
        "#Categorical Values/Object Values\n",
        "test_df1.describe(include=\"O\").T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw6Wa_azc3O1"
      },
      "outputs": [],
      "source": [
        "#missing value \n",
        "train_df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqyFqI49KH3q"
      },
      "outputs": [],
      "source": [
        "#missing value \n",
        "test_df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHyRE68hc4D7"
      },
      "outputs": [],
      "source": [
        "#duplicate train data rows  \n",
        "train_df1.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye3K-BS1KK5A"
      },
      "outputs": [],
      "source": [
        "#duplicate test data rows \n",
        "test_df1.duplicated().sum()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations**: \n",
        "1. The dataset is full of zeros \n",
        "2. We are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column\n",
        "3. The dataset has 202 unique Columns and 400,000 rows\n",
        "4. The dataset has 0 missing value \n",
        "5. The dataset has 0 duplicate rows"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MwxfU6bx3moq"
      },
      "source": [
        "### Checking for Data Uniqueness ... again "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx_C7Tqs3moq"
      },
      "outputs": [],
      "source": [
        "#checking columns with one unqiue value in train and test data \n",
        "#check unique data in the column \n",
        "unique_df = train_df1.nunique().reset_index()\n",
        "unique_df.columns = [\"col_name\", \"unique_count\"]\n",
        "unique_df = unique_df.sort_values(\"unique_count\")\n",
        "unique_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zZtA_r3mor"
      },
      "outputs": [],
      "source": [
        "#as we can see there column with any one unique value present\n",
        "#lets print the no of columns with 1 unique values \n",
        "constant_col = unique_df[unique_df[\"unique_count\"]==1]\n",
        "constant_col.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-Nxn9uf3mor"
      },
      "outputs": [],
      "source": [
        "#remove this constant col \n",
        "print('Original Shape of Train Dataset {}'.format(train_df1.shape))\n",
        "train_df1.drop(constant_col.col_name.tolist(), axis = 1, inplace = True)\n",
        "print('Shape after dropping Constant Columns from Train Dataset {}'.format(train_df1.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuvXS6ki3mor"
      },
      "outputs": [],
      "source": [
        "#checking columns with one unqiue value in train and test data \n",
        "#check unique data in the column \n",
        "unique_df = test_df1.nunique().reset_index()\n",
        "unique_df.columns = [\"col_name\", \"unique_count\"]\n",
        "unique_df = unique_df.sort_values(\"unique_count\")\n",
        "unique_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhvVR8z3mos"
      },
      "outputs": [],
      "source": [
        "#as we can see there column with any one unique value present\n",
        "#lets print the no of columns with 1 unique values \n",
        "constant_col = unique_df[unique_df[\"unique_count\"]==1]\n",
        "constant_col.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsGfDweP3mos"
      },
      "outputs": [],
      "source": [
        "#remove this constant col \n",
        "print('Original Shape of Train Dataset {}'.format(test_df1.shape))\n",
        "test_df1.drop(constant_col.col_name.tolist(), axis = 1, inplace = True)\n",
        "print('Shape after dropping Constant Columns from Train Dataset {}'.format(test_df1.shape))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations**: \n",
        "1. The dataset is f..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fpel3uUXOb8S"
      },
      "source": [
        "### Dealing with sparse data "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XVHPPmAvPMfx"
      },
      "source": [
        "Sparse data means that there are many gaps present in the data being recorded. \n",
        "As we saw in our train and test data most of the dataset values are zeros "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_pN7A1qOa93"
      },
      "outputs": [],
      "source": [
        "# lets drop the sparse data \n",
        "def drop_sparse_from_train_test(train_df1, test_df1):\n",
        "    column_to_drop_data_from = [x for x in train_df1.columns if not x in ['ID','target']]\n",
        "    for f in column_to_drop_data_from:\n",
        "        if len(np.unique(train_df1[f]))<2:\n",
        "            train_df1.drop(f, axis=1, inplace=True)\n",
        "            test_df1.drop(f, axis=1, inplace=True)\n",
        "    return train_df1, test_df1\n",
        "\n",
        "train_df1, test_df1 = drop_sparse_from_train_test(train_df1, test_df1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TOFWoNkvfQCf"
      },
      "source": [
        "### Checking for Feature Distribution "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxpw8uSf3mo0"
      },
      "outputs": [],
      "source": [
        "print('Distributions of the  columns in  the dataset')\n",
        "\n",
        "plt.figure(figsize=(40, 200))\n",
        "for i, col in enumerate(list(train_df1.columns)[2:]):\n",
        "    plt.subplot(50,4,i+1 ,);\n",
        "    plt.hist(train_df1[col])\n",
        "    plt.title(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqaiD0WxPdzw"
      },
      "outputs": [],
      "source": [
        "# Distribution of columns per target class\n",
        "print(\"Distribution of columns per target class\")\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(40,200));\n",
        "for i,col in enumerate(list(train_df1.columns)[2:]):\n",
        "    plt.subplot(50,4,i+1 ,);\n",
        "    sns.distplot(train_df1[train_df1['target']==0][col],hist=False,label='0',color='green');\n",
        "    sns.distplot(train_df1[train_df1['target']==1][col],hist=False,label='1',color='red');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqOM4weyfKMX"
      },
      "outputs": [],
      "source": [
        "# Distribution of the feature aganist the target variables \n",
        "# Scatter Plots, Distribution Curves\n",
        "my_colors = ['blue', 'red']\n",
        "sns.pairplot(train_df1,hue=\"target\", palette=my_colors, corner=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discretization of the Continous Target Variable "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Discretization is a technique where the continuous variable is divided into a set of discrete intervals, called bins. Each bin represents a range of values, and the data points are assigned to the bin that corresponds to their value \n",
        "* Entropy-based discretization: This method uses the concept of entropy to determine the optimal number of bins and the boundaries of each bin. This method tries to find the binning that maximizes the information gain of the target variable.\n",
        "* We chose this method of discretization because other methods like Equal width discretization: , Equal frequency discretization are affected by outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import numpy as np\n",
        "\n",
        "# create the discretizer\n",
        "est = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='quantile')\n",
        "\n",
        "# fit the discretizer to the target column data\n",
        "est.fit(train_df1['target'].values.reshape(-1, 1))\n",
        "\n",
        "# transform the target column data\n",
        "train_df1['target'] = est.transform(train_df1['target'].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQf-dHhofHCW"
      },
      "source": [
        "### Target Variable distribution "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYIiFR_VtV1J"
      },
      "outputs": [],
      "source": [
        "#target value count \n",
        "train_df1[\"target\"].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target Variable Analysis\n",
        "sns.countplot(train_df1[\"target\"], palette='Set2')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C2QrqL2qvlFl"
      },
      "source": [
        "**Observation**: The Target Column is balanced, i.e there is little to no bias present in the target feature"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9g7Fo1uark2f"
      },
      "source": [
        "### Checking for Feature Importance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKwKN1-UrjAk"
      },
      "outputs": [],
      "source": [
        "train_df1.corr(method='pearson').style.background_gradient(cmap='rocket_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoLwHColr9LO"
      },
      "source": [
        "**General Interprataion of Pearson Co-relation**\n",
        "1. **Perfect**: Near ± 1.\n",
        "2. **High**: ± 0.50 to ± 1\n",
        "3. **Moderate**: ± 0.30 to ± 0.49\n",
        "4. **Low** degree: Below + 0.2\n",
        "5. **None** : 0\n",
        "\n",
        "**Observation**: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #checking for relevant feature \n",
        "# cor =dataset.corr()\n",
        "# #Correlation with output variable\n",
        "# cor_target = abs(cor[\"target\"])\n",
        "# #Selecting highly correlated features\n",
        "# relevant_features = cor_target[cor_target>0.05]\n",
        "# relevant_features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "czbZJmQd-uCJ"
      },
      "source": [
        "#### Using Collinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# def calc_vif(X):\n",
        "#     # Calculating VIF\n",
        "#     vif = pd.DataFrame()\n",
        "#     vif[\"variables\"] = X.columns\n",
        "#     vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "#     return(vif)\n",
        "# result = calc_vif(dataset[dataset.columns.difference(['target', 'ID_code'], sort=False)])\n",
        "# result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting Our New Shiny Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We then extract the shiny clean processed datasets for modelling\n",
        "train_df1.to_csv(r'cleaned_data\\clean_train.csv', index=False)\n",
        "test_df1.to_csv(r'cleaned_data\\clean_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading the clean train dataset\n",
        "clean_train = pd.read_csv(\"./cleaned_data/clean_train.csv\")\n",
        "print(f'The shape of the dataset is: {clean_train.shape}')\n",
        "clean_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading the clean test dataset\n",
        "clean_test = pd.read_csv(\"./cleaned_data/clean_test.csv\")\n",
        "print(f'The shape of the dataset is: {clean_test.shape}')\n",
        "clean_test.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cAIYFyh9HOXx"
      },
      "source": [
        "## Splitting the Train Dataset into Train and Test for the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fshPv2srohIT"
      },
      "outputs": [],
      "source": [
        "# Select main columns to be used in training\n",
        "main_cols = clean_train.columns.difference(['ID', 'target'])\n",
        "X = clean_train[main_cols]\n",
        "y = clean_train.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DyLlEYVovrR"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ORZ8e2DINQFM"
      },
      "source": [
        "## Testing Different Classifier Algorithms\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uOxVnNfbNNfd"
      },
      "source": [
        "### 1. Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhzqgt63NAFd"
      },
      "outputs": [],
      "source": [
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_lr = model_lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4IGBtm3NElM"
      },
      "outputs": [],
      "source": [
        "print('Accuracy is : ', accuracy_score(y_test, y_pred_lr))\n",
        "print(f'F1 score on the X_test is: {f1_score(y_test, y_pred_lr)}')\n",
        "print(' recall:', recall_score(y_test, y_pred_lr))\n",
        "print(' precision:',precision_score(y_test, y_pred_lr))\n",
        "print('Area under the ROC curve:' , roc_auc_score(y_test, y_pred_lr))\n",
        "confusion = confusion_matrix(y_test, y_pred_lr)\n",
        "print(f'Confusion Matrix on the X_test is:\\n {confusion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL2_PDuYNG3B"
      },
      "source": [
        "**F1 Scrore Board**\n",
        "\n",
        "1.   Logistic Regression :"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "575e52FTNXK8"
      },
      "source": [
        "### 2. LGBM CLassifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52cGgl09NTud"
      },
      "outputs": [],
      "source": [
        "model_lgbm = LGBMClassifier()\n",
        "model_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lgbm = model_lgbm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RaefA8NNakv"
      },
      "outputs": [],
      "source": [
        "print('Accuracy is : ', accuracy_score(y_test, y_pred_lgbm))\n",
        "print(f'F1 score on the X_test is: {f1_score(y_test, y_pred_lgbm)}')\n",
        "print(' recall:', recall_score(y_test, y_pred_lgbm))\n",
        "print(' precision:',precision_score(y_test, y_pred_lgbm))\n",
        "print('Area under the ROC curve:' , roc_auc_score(y_test, y_pred_lgbm))\n",
        "confusion = confusion_matrix(y_test, y_pred_lgbm)\n",
        "print(f'Confusion Matrix on the X_test is:\\n {confusion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rekpz4MHNgeK"
      },
      "source": [
        "**F1 Scrore Board**\n",
        "\n",
        "1.   LGBMClassifier : \n",
        "2.   Logistic Regression : \n",
        "\n",
        "Improved by ****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_6oimxNjGx"
      },
      "source": [
        "### 3. Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzfxhsZiNly1"
      },
      "outputs": [],
      "source": [
        "main_cols = clean_train.columns.difference(['ID_code','target'])\n",
        "X = clean_train[main_cols]\n",
        "y = clean_train.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RicnKT21NoLc"
      },
      "outputs": [],
      "source": [
        "model_rf = RandomForestClassifier(criterion='entropy')   \n",
        "model_rf.fit(X_train,y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_rf = model_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH1U-rxoNsLz"
      },
      "outputs": [],
      "source": [
        "# Check accuracy, F1 score and Confusion Matrix\n",
        "\n",
        "print('Accuracy is : ', accuracy_score(y_test, y_pred_rf))\n",
        "print(f'F1 score on the X_test is: {f1_score(y_test, y_pred_rf)}')\n",
        "print(' recall:', recall_score(y_test, y_pred_rf))\n",
        "print(' precision:',precision_score(y_test, y_pred_rf))\n",
        "print('Area under the ROC curve:' , roc_auc_score(y_test, y_pred_rf))\n",
        "confusion = confusion_matrix(y_test, y_pred_rf)\n",
        "print(f'Confusion Matrix on the X_test is:\\n {confusion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYb4vW7DNtiu"
      },
      "source": [
        "**F1 Scrore Board**\n",
        "\n",
        "1.   RandomForestClassifier :\n",
        "2.   LGBMClassifier :\n",
        "3.   Logistic Regression :\n",
        "\n",
        "Improved by ****\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4S554hLRNxM7"
      },
      "source": [
        "### 4. Gradient Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFA1JSNENtPl"
      },
      "outputs": [],
      "source": [
        "model_gb = GradientBoostingClassifier(\n",
        "    n_estimators = 400,\n",
        "    learning_rate = 1.0,\n",
        "    min_samples_leaf = 10,\n",
        "    subsample = 1.0,\n",
        ")\n",
        "model_gb.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_gb = model_gb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TYgRxC0N3wX"
      },
      "outputs": [],
      "source": [
        "# Check accuracy, F1 score and Confusion Matrix\n",
        "\n",
        "print('Accuracy is : ', accuracy_score(y_test, y_pred_gb))\n",
        "print(f'F1 score on the X_test is: {f1_score(y_test, y_pred_gb)}')\n",
        "print(' recall:', recall_score(y_test, y_pred_gb))\n",
        "print(' precision:',precision_score(y_test, y_pred_gb))\n",
        "print('Area under the ROC curve:' , roc_auc_score(y_test, y_pred_gb))\n",
        "confusion = confusion_matrix(y_test, y_pred_gb)\n",
        "print(f'Confusion Matrix on the X_test is:\\n {confusion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbKrINMN5wA"
      },
      "source": [
        "**F1 Scrore Board**\n",
        "\n",
        "1.   Gradient Classifier :\n",
        "2.   RandomForestClassifier :\n",
        "3.   LGBMClassifier :\n",
        "4.   Logistic Regression :\n",
        "\n",
        "Improved by ****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u87C2We1N7MC"
      },
      "source": [
        "### XGBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKEZ_NHrOCHV"
      },
      "outputs": [],
      "source": [
        "model_xg = XGBClassifier()\n",
        "model_xg.fit(X_train, y_train)\n",
        "\n",
        "#  Prediction\n",
        "y_pred_xg = model_xg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Accuracy is : ', accuracy_score(y_test, y_pred_xg))\n",
        "print(f'F1 score on the X_test is: {f1_score(y_test, y_pred_xg)}')\n",
        "print(' recall:', recall_score(y_test, y_pred_xg))\n",
        "print(' precision:',precision_score(y_test, y_pred_xg))\n",
        "print('Area under the ROC curve:' , roc_auc_score(y_test, y_pred_xg))\n",
        "confusion = confusion_matrix(y_test, y_pred_xg)\n",
        "print(f'Confusion Matrix on the X_test is:\\n {confusion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuoGqrnTOE8f"
      },
      "source": [
        "**F1 Scrore Board**\n",
        "\n",
        "1.   Gradient Classifier :\n",
        "2.   RandomForestClassifier :\n",
        "3.   LGBMClassifier :\n",
        "4.   Logistic Regression :\n",
        "5.   XGBoost Classifier :\n",
        "\n",
        "Improved by ****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PRgwMwyPOHWo"
      },
      "source": [
        "### Ensemble Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbA4Yi-ROEKR"
      },
      "outputs": [],
      "source": [
        "# Making the final model using voting classifier\n",
        "final_model = VotingClassifier(\n",
        "\testimators=[('lr', model_lr), ('lgbm', model_lgbm)], voting='soft')\n",
        "\n",
        "# training all the model on the train dataset\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# predicting the output on the test dataset\n",
        "pred_final = final_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# printing log loss between actual and predicted value\n",
        "print(log_loss(y_test, pred_final))\n",
        "print('Accuracy is : ', accuracy_score(y_test, pred_final))\n",
        "print(f'F1 score on the X_test is: {f1_score(y_test, pred_final)}')\n",
        "print(' recall:', recall_score(y_test, pred_final))\n",
        "print(' precision:',precision_score(y_test, pred_final))\n",
        "print('Area under the ROC curve:' , roc_auc_score(y_test, pred_final))\n",
        "confusion = confusion_matrix(y_test, pred_final)\n",
        "print(f'Confusion Matrix on the X_test is:\\n {confusion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P24K88qsrXVA"
      },
      "source": [
        "**F1 Scrore Board**\n",
        "\n",
        "1.   Gradient Classifier :\n",
        "2.   RandomForestClassifier :\n",
        "3.   LGBMClassifier :\n",
        "4.   Logistic Regression :\n",
        "5.   XGBoost Classifier :\n",
        "6.   Ensemble :\n",
        "\n",
        "Improved by ****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oTvCR1sEIyK5"
      },
      "source": [
        "## Predicting The Test Dataset with our Star Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make prediction on the test set\n",
        "test_df = test_df[main_cols]\n",
        "predictions = model_lgbm.predict(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sample_sub = pd.read_csv('../data/sample_submission.csv')\n",
        "# Create a submission file\n",
        "sub_file = sample_sub.copy()\n",
        "sub_file.predictions = predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the distribution of our predictions\n",
        "sns.countplot(sub_file.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_file.to_csv('Unprocessed_Lr_Submission.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JnWK0O1TKHJG"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "36be0d2bf4bb6007abf66e79295eb6ba1209eff99d4539ea8facd3206f5f7add"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
